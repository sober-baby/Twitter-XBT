{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"CryptoLM/BTC-USDT\"\n",
    "raw_train = load_dataset(ds_name, split=\"train[:2%]\")  # First 5% of data\n",
    "raw_test  = load_dataset(ds_name, split=\"train[2%:3%]\")  # Next 1% of data\n",
    "\n",
    "\n",
    "print(raw_train)\n",
    "print(raw_test)\n",
    "\n",
    "WINDOW_SIZE = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(examples, window_size=3):\n",
    "    \"\"\"\n",
    "    For each example in 'examples', build sequences of length 'window_size'.\n",
    "    Each sequence is a textual encoding of the numeric features from t-window_size+1 to t.\n",
    "    The label is open_{t+1}.\n",
    "    \"\"\"\n",
    "    # We'll access arrays of each column\n",
    "    opens = examples[\"open\"]\n",
    "    highs = examples[\"high\"]\n",
    "    lows  = examples[\"low\"]\n",
    "    closes = examples[\"close\"]\n",
    "    \n",
    "    # You might want to include more features like volume, RSI, etc. \n",
    "    # Just retrieve them from examples[...] similarly.\n",
    "    \n",
    "    # The result lists:\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(opens) - window_size):\n",
    "        # Build the input from [i, i+1, ..., i+window_size-1]\n",
    "        seq_texts = []\n",
    "        for w in range(window_size):\n",
    "            idx = i + w\n",
    "            seq_texts.append(\n",
    "                f\"(t-{window_size - w - 1}): \"\n",
    "                f\"open={opens[idx]}, high={highs[idx]}, low={lows[idx]}, close={closes[idx]}\"\n",
    "            )\n",
    "        # Combine all lines into one long text\n",
    "        combined_text = \" | \".join(seq_texts)\n",
    "        \n",
    "        # The label: open_{(i + window_size)} \n",
    "        # i + window_size is the day after the last day in the window\n",
    "        # We treat that as float\n",
    "        next_open = float(opens[i + window_size])\n",
    "        \n",
    "        sequences.append(combined_text)\n",
    "        labels.append(next_open)\n",
    "        \n",
    "    return {\"input_text\": sequences, \"label\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slid = raw_train.map(\n",
    "    create_sequences,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"window_size\": WINDOW_SIZE},\n",
    "    remove_columns=raw_train.column_names\n",
    ")\n",
    "test_slid = raw_test.map(\n",
    "    create_sequences,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"window_size\": WINDOW_SIZE},\n",
    "    remove_columns=raw_test.column_names\n",
    ")\n",
    "\n",
    "print(train_slid[0][\"input_text\"])\n",
    "print(train_slid[0][\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Sometimes you need to define a pad token explicitly for certain models\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\", \n",
    "        max_length=128  # you might adjust based on window size\n",
    "    )\n",
    "\n",
    "train_tokenized = train_slid.map(tokenize_fn, batched=True)\n",
    "test_tokenized  = test_slid.map(tokenize_fn,  batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_label_col(example):\n",
    "    return {\"labels\": example[\"label\"]}\n",
    "\n",
    "train_tokenized = train_tokenized.map(rename_label_col, remove_columns=[\"label\", \"input_text\"])\n",
    "test_tokenized  = test_tokenized.map(rename_label_col, remove_columns=[\"label\", \"input_text\"])\n",
    "\n",
    "train_tokenized.set_format(\"torch\")\n",
    "test_tokenized.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=1  # crucial for regression\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_metric = evaluate.load(\"mse\")\n",
    "mae_metric = evaluate.load(\"mae\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # predictions shape: (batch_size, 1), so flatten\n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    # MSE returns a dict, e.g. {\"mean_squared_error\": 0.5}\n",
    "    mse_result = mse_metric.compute(predictions=predictions, references=labels)\n",
    "    # Extract the float\n",
    "    mse_value = mse_result[\"mse\"]\n",
    "    \n",
    "    # MAE returns a dict, e.g. {\"mean_absolute_error\": 0.3}\n",
    "    mae_result = mae_metric.compute(predictions=predictions, references=labels)\n",
    "    mae_value = mae_result[\"mae\"]\n",
    "    \n",
    "    # Compute RMSE\n",
    "    rmse_value = mse_value ** 0.5\n",
    "    \n",
    "    return {\n",
    "        \"mse\": mse_value,\n",
    "        \"rmse\": rmse_value,\n",
    "        \"mae\": mae_value,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned-btc-regression\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,  # We'll pick the best by default on val_loss\n",
    "    metric_for_best_model=\"mse\",  # or \"rmse\"\n",
    "    greater_is_better=False,\n",
    "    # GPU usage\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"  # or \"wandb\"/\"tensorboard\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we want to predict after the last window in the test set\n",
    "sample = test_tokenized[-1]\n",
    "\n",
    "# The Trainer expects a batch, so replicate or wrap in list\n",
    "inputs = {k: torch.tensor(sample[k]).unsqueeze(0).to(model.device) \n",
    "          for k in [\"input_ids\", \"attention_mask\"]}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "prediction = outputs.logits.item()  # shape [1,1]\n",
    "print(\"Predicted next-day open:\", prediction)\n",
    "\n",
    "actual = test_slid[-1][\"label\"]\n",
    "print(\"Actual next-day open:\", actual)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
